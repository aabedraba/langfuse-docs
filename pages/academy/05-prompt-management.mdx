## **Module 5:** Prompt Management / Engineering

After **evaluating** your LLM workflows, you'll often find areas to improveâ€”whether by refining prompts, changing models, or updating tool definitions. In this module, we'll look at how to **apply** those findings through systematic prompt management and iterative experimentation. 

<Frame className="my-10" fullWidth>
![Prompt Management](/images/academy/m5-prompt-management.png)
</Frame>

### Prompt Management

Effective prompt management keeps your LLM application agile, reproducible, and collaborationâ€‘friendly. Without a systematic way to store, version, and experiment with prompts, seemingly minor text tweaks can break customer flows or silently inflate costs. This module shows why prompt management matters, introduces core prompting strategies, and walks through Langfuse's prompt store and experiment features.

#### **Why Prompt Management?**

* **Reproducibility & rollback** â€“ Prompts evolve faster than code; versioning prevents silent regressions and enables instant rollback when quality dips.  
* **Governance & auditability** â€“ Regulated domains (health, finance, legal) must trace which exact wording produced an outputâ€¯.  
* **Collaboration across teams** â€“ Product managers and domain experts often iterate on prompts; a central prompt store avoids "prompt spaghetti" in codebases.  
* **A/B testing & optimisation** â€“ Structured experiments reveal cost/quality tradeâ€‘offs and prevent prompt drift.  
* **Common pitfalls** â†’ brittle hardâ€‘coded strings, shadow prompts living in notebooks, unclear ownership, and uncontrolled temperature/parameter changes.

#### **Introduction to Common Prompting Strategies**

If you are new to prompting, here is a rough overview of different strategies that can improve the performance of your application. For more advanced prompting strategies, we collected some high-quality resources [here](https://langfuse.com/library#prompting). 

| Strategy | Core Idea | When to Use | Key Risk |
| :---- | :---- | :---- | :---- |
| **Zeroâ€‘Shot** | Provide only task instructions; rely on model generality | Fast prototyping | Ambiguous outputs |
| **Fewâ€‘Shot / Inâ€‘Context** | Add 1â€‘5 examples to steer style or structure | Structured outputs, dataâ€‘sparse tasks | Higher token cost |
| **Chainâ€‘ofâ€‘Thought (CoT)** | Ask model to reason stepâ€‘byâ€‘step before final answer | Complex reasoning tasks | Latency, leak chain to user |
| **Role Prompting** | Assign the model a persona or professional role | Tone control, empathy | Overâ€‘constrained style |
| **Retrievalâ€‘Augmented Generation (RAG)** | Dynamically inject retrieved docs into contextâ€¯ | Fresh, sourceâ€‘grounded answers | Retrieval latency |
| **Prefixâ€‘Tuning / Systemâ€‘Content Split** | Separate stable system message from dynamic user messageâ€¯ | Multiâ€‘turn chat apps | Duplication across turns |

#### **Using Prompt Management in Langfuse**

Langfuse offers a **Prompt Store** where prompts live as firstâ€‘class versioned entities; each version links to the traces it produced for instant cost/quality analysis. You can:

1. **Create & edit prompts** via UI, API, or SDK without redeploying the app.  
2. **Pin versions to environments** (e.g., `prod` vs `staging`) to avoid accidental crossâ€‘contamination.  
3. **Run A/B experiments** by splitting traffic across prompt versions and comparing metrics directly in Langfuse dashboards.  
4. **Link prompts to evaluations** so that score regressions surface next to the exact text diffâ€¯.

To get started managing prompts in Langfuse, check out our [prompt management documentation](https://langfuse.com/docs/prompts/get-started).

#### **Prompt Engineering Loop**

For most LLM applications, it is important to involve domain experts in the design of LLM prompts. 

1. **Define success criteria** â€“ domain stakeholders translate policy/compliance or UX goals into measurable metrics (accuracy, tone, latency).  
2. **Draft baseline prompt** â€“ engineer assembles initial system \+ user prompt following chosen strategy.  
3. **Share in Langfuse Prompt Experiments** â€“ nonâ€‘technical reviewers comment, annotate token costs, and suggest edits in the UI (no Git access needed)â€¯.  
4. **Run controlled experiment** â€“ split traffic 80/20 between baseline and candidate; Langfuse autoâ€‘collects costs, eval scores, and feedbackâ€¯.  
5. **Review & decide** â€“ crossâ€‘functional meeting reviews dashboards; if candidate wins on KPIs, promote to `prod`.  
6. **Postâ€‘mortem & document** â€“ every prompt update autoâ€‘links to traces and eval runs, building an audit trailâ€¯.


<Callout type="info" emoji="ðŸ“š">
Further reading:

- **Learn Prompting**, [docs page](https://learnprompting.org/docs)
- **The Prompt Report: A Systematic Survey of Prompting Techniques**, [paper on arxiv](https://arxiv.org/pdf/2406.06608), [summary in tweets](https://x.com/learnprompting/status/1800931910404784380)
- **How to prompt o1** (o1 isnâ€™t a chat model â€“ and thatâ€™s the point), [blog post](https://www.latent.space/p/o1-skill-issue), _by Ben Hylak_

</Callout>




















