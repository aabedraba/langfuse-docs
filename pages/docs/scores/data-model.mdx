---
title: Getting Started with LLM Evaluation in Langfuse
description: Langfuse (open source) helps evaluate LLM applications. Commonly used to measure quality, tonality, factual accuracy, completeness, and relevance.
---

# Evaluation Data Model

Evaluation is critical for the LLM Application Development workflow. Also, it is very specific to a certain use case and application. Thus, the evaluation data model in Langfuse is flexible to represent any evaluation metric. It is used across the evalaution methods outlined in the [overview](/docs/scores/overview).

import ScoreDataModel from "@/components-mdx/score-data-model.mdx";

<ScoreDataModel />

<details>
<summary>How are scores used across Langfuse?</summary>

Scores can be used in multiple ways across Langfuse:

1. Displayed on trace/session to provide a quick overview
2. Segment all execution traces by scores to e.g. find all traces with a low quality score
3. Analytics: Detailed score reporting with drill downs into use cases and user segments

</details>

<details>
<summary>Frequently used scores</summary>

Scores in Langfuse are adaptable (it is just a `name`) and designed to cater to the unique requirements of specific LLM applications. They typically serve to measure the following aspects:

- Quality
  - Factual accuracy
  - Completeness of the information provided
  - Verification against hallucinations
- Style
  - Sentiment portrayed
  - Tonality of the content
  - Potential toxicity
- Security
  - Similarity to prevalent prompt injections
  - Instances of model refusals (e.g., as a language model, ...)

</details>

## Score Configuration

If you'd like to ensure that your scores follow a specific schema, you can define a `score config` in the Langfuse UI or via our API.

A score config includes:

- Score name
- Data type: `NUMERIC`, `CATEGORICAL`, `BOOLEAN`
- Constraints on score value range:
  - Min/max values for numerical data types
  - Custom categories for categorical data types

Configs are immutable but can be archived (and restored anytime). Using score configs allows you to standardize your scoring schema across your team and ensure that scores are consistent and comparable for future analysis.

## Get started

You can either utilize evaluation scores in Langfuse via:

- [Programmatically via API/SDK](/docs/scores/custom)
  - Example: [User Feedback](/docs/scores/user-feedback)
  - Example: [Custom evaluation pipeline](/docs/scores/external-evaluation-pipelines)
- Managed evaluation methods in Langfuse
  - [LLM-as-a-judge](/docs/scores/model-based-evals)
  - [Human evaluation](/docs/scores/annotation)
