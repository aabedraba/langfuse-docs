---
title: Open Source Prompt Management
description: Manage and version your prompts in Langfuse (open source). When retrieved, they are cached by the Langfuse SDKs for low latency.
---

# Prompt Management

Use Langfuse to effectively **manage** and **version** your prompts. Langfuse prompt management is a **Prompt CMS** (Content Management System).

import PromptOverview from "@/components-mdx/prompt-overview-gifs.mdx";

<PromptOverview />

## What is prompt management?

**Prompt management is a systematic approach to storing, versioning and retrieving prompts in LLM applications.** Key aspects of prompt management include version control, decoupling prompts from code, monitoring, logging and optimizing prompts as well as integrating prompts with the rest of your application and tool stack.

## Why use prompt management?

> Can't I just hardcode my prompts in my application and track them in Git? Yes, well... you can and all of us have done it.

Typical benefits of using a CMS apply here:

- Decoupling: deploy new prompts without redeploying your application.
- Non-technical users can create and update prompts via Langfuse Console.
- Quickly rollback to a previous version of a prompt.
- Compare different prompt versions side-by-side.

Platform benefits:

- Track performance of prompt versions in [Langfuse Tracing](/docs/tracing).

Performance benefits compared to other implementations:

- No latency impact after first use of a prompt due to client-side caching and asynchronous cache refreshing.
- Support for text and chat prompts.
- Edit/manage via UI, SDKs, or API.

## Prompt Engineering FAQ

import {
  Accordion,
  AccordionItem,
  AccordionTrigger,
  AccordionContent,
} from "@/components/ui/accordion";

<Accordion>
<AccordionItem value="what-is-prompt-engineering">
<AccordionTrigger>What is prompt engineering?</AccordionTrigger>
<AccordionContent>

Prompt engineering is the practice of designing and refining prompts to effectively communicate with and guide AI models, particularly large language models (LLMs), to produce desired outputs.

Prompt engineering involves:

1. Crafting clear and specific instructions for AI models
2. Utilizing techniques like role assignment, few-shot prompting, and chain-of-thought reasoning
3. Optimizing prompts for different applications such as text generation, summarization, and problem-solving
4. Understanding the capabilities and limitations of AI models
5. Iteratively refining prompts to improve output quality and reliability
6. Applying various advanced techniques like self-consistency, generated knowledge, and least-to-most prompting
7. Considering ethical implications and potential biases in prompt design

By using Langfuse prompt management, you can version and manage your prompts collaboratively to execute on the steps above.

Recommended readings:

- [Learn Prompting Documentation](https://learnprompting.org/docs)
- [The Prompt Report](https://arxiv.org/abs/2406.06608)
- [Prompting Fundamentals and How to Apply them Effectively](https://eugeneyan.com/writing/prompting/)

</AccordionContent>
</AccordionItem>
<AccordionItem value="how-to-measure-prompt-performance">
<AccordionTrigger>How to measure prompt performance?</AccordionTrigger>
<AccordionContent>

Depending on the scale of your experiments, there are different approaches you can take:

1. [Playground](/docs/playground) for single prompt experiments in UI.
2. [Releases and Versioning](/docs/tracing-features/releases-and-versioning) for A/B testing and structured experiments in production.
3. [Datasets](/docs/datasets/overview) for offline/dev benchmarking prompts or applications on a set of reference inputs.

</AccordionContent>
</AccordionItem>
</Accordion>

## Performance measurement of inital fetch (empty client-side cache)

We measured the execution time of the following snippet with fully disabled caching.

```python
prompt = langfuse.get_prompt("perf-test")
prompt.compile(input="test")
```

Results from 1000 sequential executions in a local jupyter notebook using Langfuse Cloud (includes network latency):

<div className="sm:grid sm:grid-cols-2 gap-4">

<Frame className="max-w-md">
  ![Performance Chart](/images/docs/prompt-performance-chart.png)
</Frame>

```
count  1000.000000
mean      0.178465 sec
std       0.058125 sec
min       0.137314 sec
25%       0.161333 sec
50%       0.165919 sec
75%       0.171736 sec
max       0.687994 sec
```

</div>
