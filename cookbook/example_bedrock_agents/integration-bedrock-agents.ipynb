{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing and Evaluating Bedrock Agents with Langfuse\n",
    "\n",
    "This notebook demonstrates how to integrate [Langfuse](https://langfuse.com) for tracing and evaluation of [Bedrock Agents](https://aws.amazon.com/bedrock/agents/). Bedrock Agents enable you to build generative AI applications that can perform tasks, orchestrate calls to company systems, and access knowledge sources.\n",
    "\n",
    "We will cover:\n",
    "1.  **Setup**: Installing necessary packages and configuring AWS and Langfuse credentials.\n",
    "2.  **Basic Agent Invocation**: Interacting with a Bedrock Agent.\n",
    "3.  **Tracing with OpenTelemetry**: Sending detailed traces of agent interactions to Langfuse.\n",
    "4.  **Offline Evaluation**: Using Langfuse Datasets to systematically test your agent and compare performance across different versions or configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup - Install Dependencies\n",
    "\n",
    "First, let's install the necessary Python packages. This includes `boto3` for interacting with AWS services, OpenTelemetry packages for tracing, and `langfuse` for observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3\n",
    "%pip install opentelemetry-api\n",
    "%pip install opentelemetry-sdk\n",
    "%pip install opentelemetry-exporter-otlp\n",
    "%pip install wrapt\n",
    "%pip install langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the required libraries for interacting with AWS, handling data, unique identifiers, and setting up Langfuse tracing via our custom `core` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import uuid\n",
    "import json\n",
    "from core.timer_lib import timer\n",
    "from core import instrument_agent_invocation, flush_telemetry\n",
    "import os\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Interacting with AWS Bedrock Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Configure AWS Credentials\n",
    "\n",
    "To interact with your AWS Bedrock Agent, you need to configure your AWS credentials. \n",
    "\n",
    "**IMPORTANT**: Replace the placeholder values below with your actual `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_DEFAULT_REGION`. \n",
    "For more information on AWS credentials and best practices (like using IAM roles), refer to the [AWS Boto3 Credentials Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIAWLHGBJNACCDMQBFP\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Specify Bedrock Agent Details\n",
    "\n",
    "Provide the `agent_id` and `agent_alias_id` for the Bedrock Agent you want to interact with. You can find these in your AWS Bedrock console after creating and deploying an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_id = \"VDD9470BPM\"  # <- Configure your Bedrock Agent ID\n",
    "agent_alias_id = \"TSTALIASID\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Initialize Bedrock Agent Runtime Client\n",
    "\n",
    "We create a `boto3` client for the `bedrock-agent-runtime` service. This client will be used to invoke the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create the client to invoke Agents in Amazon Bedrock:\n",
    "br_agents_runtime = boto3.client(\"bedrock-agent-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4: Test Basic Agent Invocation\n",
    "\n",
    "Let's perform a simple invocation to ensure we can connect to the agent and receive a response. We'll use a unique session ID for this initial test. A `sessionId` helps maintain context across multiple turns in a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to invoke alias TSTALIASID of agent VDD9470BPM...\n",
      "âœ… Got response\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trying to invoke alias {agent_alias_id} of agent {agent_id}...\")\n",
    "agent_resp = br_agents_runtime.invoke_agent(\n",
    "    agentAliasId=agent_alias_id,\n",
    "    agentId=agent_id,\n",
    "    inputText=\"Hello!\",\n",
    "    sessionId=\"dummy-session\",\n",
    ")\n",
    "if \"completion\" in agent_resp:\n",
    "    print(\"âœ… Got response\")\n",
    "else:\n",
    "    raise ValueError(f\"No 'completion' in agent response:\\n{agent_resp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tracing Agent Invocations with Langfuse\n",
    "\n",
    "Langfuse provides detailed tracing for your LLM applications. We'll use OpenTelemetry (OTEL) to send trace data from our Bedrock Agent interactions to Langfuse. This allows us to monitor performance, debug issues, and understand the agent's behavior.\n",
    "\n",
    "Refer to the [Langfuse OpenTelemetry documentation](/docs/opentelemetry/get-started) for more details on setting up OTEL with Langfuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Configure Langfuse Credentials & OTEL Variables\n",
    "\n",
    "Set up your Langfuse public key, secret key, and host. You can find your API keys in your Langfuse project settings ([Cloud](https://cloud.langfuse.com) or your self-hosted instance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-d4bf91ce-b51a-45ea-bda8-cf2368e06af5\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-16e924ad-f614-4b8c-844d-1602e8a9e764\" \n",
    "\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    "\n",
    "# For Langfuse specifically but you can add any other observability provider:\n",
    "os.environ[\"OTEL_SERVICE_NAME\"] = 'Langfuse'\n",
    "os.environ[\"DEPLOYMENT_ENVIRONMENT\"] = \"dev\"\n",
    "project_name = \"agent-observability\"\n",
    "environment = \"dev\"\n",
    "\n",
    "LANGFUSE_AUTH = base64.b64encode(\n",
    "    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n",
    ").decode()\n",
    "\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = os.environ.get(\"LANGFUSE_HOST\") + \"/api/public/otel/v1/traces\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Define Instrumented Agent Invocation Function\n",
    "\n",
    "We define a wrapper function `invoke_bedrock_agent_instrumented`. This function is decorated with `@instrument_agent_invocation`, which handles the OpenTelemetry span creation and export to Langfuse. This decorator automatically captures details about the agent's execution, including inputs, outputs, metadata, and any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@instrument_agent_invocation\n",
    "def invoke_bedrock_agent(\n",
    "    inputText: str, agentId: str, agentAliasId: str, sessionId: str, **kwargs\n",
    "):\n",
    "    \"\"\"Invoke a Bedrock Agent with instrumentation for Langfuse.\"\"\"\n",
    "    # Create Bedrock client\n",
    "    bedrock_rt_client = boto3.client(\"bedrock-agent-runtime\")\n",
    "    use_streaming = kwargs.get(\"streaming\", False)\n",
    "    invoke_params = {\n",
    "        \"inputText\": inputText,\n",
    "        \"agentId\": agentId,\n",
    "        \"agentAliasId\": agentAliasId,\n",
    "        \"sessionId\": sessionId,\n",
    "        \"enableTrace\": True,  # Required for instrumentation\n",
    "    }\n",
    "\n",
    "    # Add streaming configurations if needed\n",
    "    if use_streaming:\n",
    "        invoke_params[\"streamingConfigurations\"] = {\n",
    "            \"applyGuardrailInterval\": 10,\n",
    "            \"streamFinalResponse\": True,\n",
    "        }\n",
    "    response = bedrock_rt_client.invoke_agent(**invoke_params)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Invoke Agent with Langfuse Tracing\n",
    "\n",
    "Now, let's call our instrumented function. We can pass additional metadata that will be captured by Langfuse, such as `trace_id` (to group related operations), `userId` (to track interactions per user), `tags` (for categorization), `project_name`, and `environment`. This metadata enriches the traces in Langfuse, making them easier to search, filter, and analyze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Error during agent invocation: AWSHTTPSConnectionPool(host='bedrock-agent-runtime.us-east-1.amazonaws.com', port=443): Read timed out.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jannik/Documents/GitHub/langfuse-docs/.venv/lib/python3.13/site-packages/urllib3/response.py\", line 754, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/jannik/Documents/GitHub/langfuse-docs/.venv/lib/python3.13/site-packages/urllib3/response.py\", line 1219, in read_chunked\n",
      "    self._update_chunk_length()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/jannik/Documents/GitHub/langfuse-docs/.venv/lib/python3.13/site-packages/urllib3/response.py\", line 1138, in _update_chunk_length\n",
      "    line = self._fp.fp.readline()  # type: ignore[union-attr]\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py\", line 1304, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py\", line 1138, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jannik/Documents/GitHub/langfuse-docs/cookbook/example_bedrock_agents/core/agent.py\", line 495, in wrapper\n",
      "    for event in response[\"completion\"]:\n",
      "                 ~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/Users/jannik/Documents/GitHub/langfuse-docs/.venv/lib/python3.13/site-packages/botocore/eventstream.py\", line 591, in __iter__\n",
      "    for event in self._event_generator:\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jannik/Documents/GitHub/langfuse-docs/.venv/lib/python3.13/site-packages/botocore/eventstream.py\", line 598, in _create_raw_event_generator\n",
      "    for chunk in self._raw_stream.stream():\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/jannik/Documents/GitHub/langfuse-docs/.venv/lib/python3.13/site-packages/urllib3/response.py\", line 1063, in stream\n",
      "    yield from self.read_chunked(amt, decode_content=decode_content)\n",
      "  File \"/Users/jannik/Documents/GitHub/langfuse-docs/.venv/lib/python3.13/site-packages/urllib3/response.py\", line 1202, in read_chunked\n",
      "    with self._error_catcher():\n",
      "         ~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\", line 162, in __exit__\n",
      "    self.gen.throw(value)\n",
      "    ~~~~~~~~~~~~~~^^^^^^^\n",
      "  File \"/Users/jannik/Documents/GitHub/langfuse-docs/.venv/lib/python3.13/site-packages/urllib3/response.py\", line 759, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\") from e  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "urllib3.exceptions.ReadTimeoutError: AWSHTTPSConnectionPool(host='bedrock-agent-runtime.us-east-1.amazonaws.com', port=443): Read timed out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': \"AWSHTTPSConnectionPool(host='bedrock-agent-runtime.us-east-1.amazonaws.com', port=443): Read timed out.\", 'exception': \"AWSHTTPSConnectionPool(host='bedrock-agent-runtime.us-east-1.amazonaws.com', port=443): Read timed out.\"}\n"
     ]
    }
   ],
   "source": [
    "# Generate a custom trace ID\n",
    "trace_id = str(uuid.uuid4())\n",
    "\n",
    "# Single invocation that works for both streaming and non-streaming\n",
    "response = invoke_bedrock_agent(\n",
    "    inputText=\"Hi there!\",\n",
    "    agentId=\"VDD9470BPM\",\n",
    "    agentAliasId=\"TSTALIASID\",\n",
    "    sessionId=\"session-123456789\",\n",
    "    show_traces=True,\n",
    "    SAVE_TRACE_LOGS=True,\n",
    "    userId=\"user-1234\",\n",
    "    tags=[\"bedrock-agent\", \"example\", \"development\"],\n",
    "    trace_id=trace_id,\n",
    "    project_name=\"bedrock-agent-observability\",\n",
    "    environment=\"dev\",\n",
    "    langfuse_public_key=os.environ.get('LANGFUSE_PUBLIC_KEY'),\n",
    "    langfuse_secret_key=os.environ.get('LANGFUSE_SECRET_KEY'),\n",
    "    langfuse_api_url=os.environ.get('LANGFUSE_HOST'),\n",
    "    streaming=False,\n",
    "    model_id=\"claude-3-5-sonnet-20241022\",\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "flush_telemetry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Offline Evaluation of Bedrock Agents using Langfuse Datasets\n",
    "\n",
    "While online evaluation provides live feedback, **offline evaluation** is crucial for systematically testing your agent against a benchmark dataset before deployment or during development iterations. This helps ensure quality and reliability. \n",
    "\n",
    "In a typical offline evaluation workflow with Langfuse Datasets:\n",
    "1.  You prepare a benchmark dataset in Langfuse. Each dataset item consists of an input (e.g., a question) and optionally an expected output or other metadata.\n",
    "2.  You iterate through the dataset items, running your Bedrock Agent for each input.\n",
    "3.  You link each agent execution (which is a Langfuse trace) back to the corresponding dataset item in Langfuse.\n",
    "4.  Optionally, you can add scores (e.g., for correctness, relevance) to these linked runs, either manually or using automated methods like [Model-Based Evals](/docs/scores/model-based-evals). Langfuse then enables you to compare performance across different evaluation runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Define Agent Function for Dataset Item Processing\n",
    "\n",
    "We define a function `run_agent_on_dataset_item` that takes a dataset item's input, invokes the Bedrock agent using our instrumented function `invoke_bedrock_agent_instrumented`, and returns the trace ID and the agent's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_agent(question):      \n",
    "        \n",
    "        # Generate a custom trace ID\n",
    "        trace_id = str(uuid.uuid4())\n",
    "\n",
    "        response = invoke_bedrock_agent(\n",
    "            inputText=question,\n",
    "            agentId=agentId,\n",
    "            agentAliasId=agentAliasId,\n",
    "            sessionId=sessionId,\n",
    "            show_traces=True,\n",
    "            SAVE_TRACE_LOGS=True,\n",
    "            userId=userId,\n",
    "            tags=tags,\n",
    "            trace_id=trace_id,\n",
    "            project_name=project_name,\n",
    "            environment=environment,\n",
    "            langfuse_public_key=os.environ.get('LANGFUSE_PUBLIC_KEY'),\n",
    "            langfuse_secret_key=os.environ.get('LANGFUSE_SECRET_KEY'),\n",
    "            langfuse_api_url=os.environ.get('LANGFUSE_HOST'),\n",
    "            streaming=False,\n",
    "            model_id=agent_model_id,\n",
    "        )        \n",
    "\n",
    "        return trace_id, response['extracted_completion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Run Agent on Dataset and Link Traces to Langfuse\n",
    "\n",
    "Finally, we fetch a dataset from Langfuse and then iterate through each item in the dataset. \n",
    "\n",
    "Finally, `langfuse.flush()` ensures all trace data and linkage information are sent to Langfuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()\n",
    "\n",
    "dataset = langfuse.get_dataset('dataset-restaurant-agent')\n",
    "\n",
    "for item in dataset.items:\n",
    "\n",
    "    trace_id, output = my_agent(item.input[\"text\"])\n",
    "\n",
    "    # link the execution trace to the dataset item and give it a run_name\n",
    "    item.link(\n",
    "        trace_or_observation = langfuse.trace(id = trace_id),\n",
    "        run_name = \"run_test\",\n",
    "        run_description=\"my dataset run\", # optional\n",
    "        run_metadata={ \"model\": \"gpt-4.5-preview\" } # optional\n",
    "    )\n",
    "\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4: Analyze Results in Langfuse\n",
    "\n",
    "After running the evaluation, navigate to your project in Langfuse. In the 'Datasets' section, select your dataset. You will find your evaluation run listed under the 'Runs' tab for that dataset.\n",
    "\n",
    "Langfuse helps you compare these runs based on the captured traces and associated scores. You can set up [Model-Based Evals](/docs/scores/model-based-evals) in Langfuse to automatically assess aspects like correctness against expected answers (if your dataset includes them) or other quality dimensions based on LLM-as-a-judge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
