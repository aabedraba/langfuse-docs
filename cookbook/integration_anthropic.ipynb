{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"\u26a0\ufe0f Jupyter Notebook\" title: \"Integrate Anthropic with Langfuse\" sidebarTitle: \"Anthropic\" logo: \"/images/integrations/anthropic_icon.svg\" description: \"Learn how to monitor and trace Anthropic models with Langfuse to improve and debug your AI applications.\" category: \"Integrations\" -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Observability for Anthropic Models with Langfuse\n",
        "\n",
        "Anthropic provides advanced language models like Claude, known for their safety, helpfulness, and strong reasoning capabilities. By combining Anthropic's models with **Langfuse**, you can trace, monitor, and analyze your AI workloads in development and production.\n",
        "\n",
        "This notebook demonstrates **two** different ways to use Anthropic models with Langfuse:\n",
        "1. **Native Anthropic SDK with Langfuse Decorators:** Use Langfuse decorators to wrap Anthropic SDK calls for automatic tracing.\n",
        "2. **OpenAI SDK Drop-in Replacement:** Use Anthropic's OpenAI-compatible endpoints via Langfuse's OpenAI SDK wrapper.\n",
        "\n",
        "> **What is Anthropic?**  \n",
        "Anthropic is an AI safety company that develops Claude, a family of large language models designed to be helpful, harmless, and honest. Claude models excel at complex reasoning, analysis, and creative tasks.\n",
        "\n",
        "> **What is Langfuse?**  \n",
        "[Langfuse](https://langfuse.com) is an open source platform for LLM observability and monitoring. It helps you trace and monitor your AI applications by capturing metadata, prompt details, token usage, latency, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Before you begin, install the necessary packages in your Python environment:\n",
        "\n",
        "- **anthropic**: The official Anthropic Python SDK for using Claude models.\n",
        "- **openai**: Needed to call Anthropic's OpenAI-compatible endpoints.\n",
        "- **langfuse**: Required for sending trace data to the Langfuse platform.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install anthropic openai langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set Up Environment Variables\n",
        "\n",
        "Configure your **Langfuse** credentials and **Anthropic** API key as environment variables. Replace the dummy keys below with the real ones from your respective accounts.\n",
        "\n",
        " - `LANGFUSE_PUBLIC_KEY` / `LANGFUSE_SECRET_KEY`: From your Langfuse Project Settings.\n",
        " - `LANGFUSE_HOST`: `https://cloud.langfuse.com` (EU region) or `https://us.cloud.langfuse.com` (US region).\n",
        " - `ANTHROPIC_API_KEY`: Your Anthropic API key from the [Anthropic Console](https://console.anthropic.com/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Example environment variables (replace with your actual keys)\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"  # your public key\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"  # your secret key\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # or https://us.cloud.langfuse.com\n",
        "\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-...\"  # Your Anthropic API key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 1: Using Native Anthropic SDK with Langfuse Decorators\n",
        "\n",
        "Langfuse decorators provide a simple way to trace function calls and automatically capture input/output data. This approach allows you to use the native Anthropic SDK while getting full observability through Langfuse.\n",
        "\n",
        "### Steps\n",
        "1. Import the Anthropic client and Langfuse decorators.\n",
        "2. Use the `@observe()` decorator on functions that call Anthropic.\n",
        "3. Use `@langfuse_context.update_current_observation()` to pass model details and usage information.\n",
        "4. Make API calls to Anthropic as normal.\n",
        "5. View the trace in your Langfuse dashboard.\n",
        "\n",
        "**Note:** For more examples on using Langfuse decorators, see the [Langfuse Python SDK documentation](https://langfuse.com/docs/sdk/python/decorators)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from anthropic import Anthropic\n",
        "from langfuse.decorators import observe, langfuse_context\n",
        "\n",
        "# Initialize the Anthropic client\n",
        "anthropic = Anthropic(\n",
        "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@observe()\n",
        "def chat_with_claude(messages: list, model: str = \"claude-3-5-sonnet-20241022\", max_tokens: int = 1024):\n",
        "    \"\"\"Chat with Claude using the Anthropic SDK and trace with Langfuse.\"\"\"\n",
        "    \n",
        "    # Make the API call to Anthropic\n",
        "    response = anthropic.messages.create(\n",
        "        model=model,\n",
        "        max_tokens=max_tokens,\n",
        "        messages=messages\n",
        "    )\n",
        "    \n",
        "    # Update Langfuse observation with model details and usage\n",
        "    langfuse_context.update_current_observation(\n",
        "        model=model,\n",
        "        input=messages,\n",
        "        output=response.content[0].text,\n",
        "        usage={\n",
        "            \"input\": response.usage.input_tokens,\n",
        "            \"output\": response.usage.output_tokens,\n",
        "            \"total\": response.usage.input_tokens + response.usage.output_tokens\n",
        "        },\n",
        "        metadata={\n",
        "            \"model_id\": response.model,\n",
        "            \"stop_reason\": response.stop_reason\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage with decorator\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is Langfuse and how does it help with LLM observability?\"}\n",
        "]\n",
        "\n",
        "response = chat_with_claude(messages)\n",
        "print(response.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use the decorator pattern for more complex workflows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@observe()\n",
        "def analyze_text(text: str):\n",
        "    \"\"\"Analyze text using Claude with multiple steps.\"\"\"\n",
        "    \n",
        "    # Step 1: Summarize the text\n",
        "    summary = chat_with_claude([\n",
        "        {\"role\": \"user\", \"content\": f\"Summarize this text in 2 sentences: {text}\"}\n",
        "    ], max_tokens=200)\n",
        "    \n",
        "    # Step 2: Extract key points\n",
        "    key_points = chat_with_claude([\n",
        "        {\"role\": \"user\", \"content\": f\"List 3 key points from this text: {text}\"}\n",
        "    ], max_tokens=300)\n",
        "    \n",
        "    return {\n",
        "        \"summary\": summary.content[0].text,\n",
        "        \"key_points\": key_points.content[0].text\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "sample_text = \"\"\"Langfuse is an open-source LLM engineering platform that helps teams \n",
        "collaboratively debug, analyze, and iterate on their LLM applications. It provides \n",
        "detailed production traces, analytics, prompt management, and evaluation capabilities.\"\"\"\n",
        "\n",
        "result = analyze_text(sample_text)\n",
        "print(\"Summary:\", result[\"summary\"])\n",
        "print(\"\\nKey Points:\", result[\"key_points\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the request completes, **log in to your Langfuse dashboard** and look for the new trace. You will see:\n",
        "- The full conversation tree with nested observations\n",
        "- Input/output for each Claude API call\n",
        "- Token usage and costs\n",
        "- Latency metrics\n",
        "- Any metadata you've added\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Approach 2: Using OpenAI SDK Drop-in Replacement\n",
        "\n",
        "Anthropic provides OpenAI-compatible endpoints that allow you to use the OpenAI SDK to interact with Claude models. This is particularly useful if you have existing code using the OpenAI SDK that you want to switch to Claude.\n",
        "\n",
        "### Steps\n",
        "1. Import the `OpenAI` client from `langfuse.openai`.\n",
        "2. Create a client, setting `api_key` to your Anthropic API key and `base_url` to Anthropic's OpenAI-compatible endpoint.\n",
        "3. Use the client's `chat.completions.create()` method with Claude model names.\n",
        "4. View the trace in your Langfuse dashboard.\n",
        "\n",
        "**Note:** This approach requires that Anthropic has enabled OpenAI-compatible endpoints for your account. Check Anthropic's documentation for availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Langfuse OpenAI client\n",
        "from langfuse.openai import OpenAI\n",
        "\n",
        "# Create an OpenAI client pointing to Anthropic's compatible endpoint\n",
        "# Note: This assumes Anthropic provides OpenAI-compatible endpoints\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
        "    base_url=\"https://api.anthropic.com/v1/messages/openai\",  # Hypothetical endpoint\n",
        "    default_headers={\n",
        "        \"anthropic-version\": \"2023-06-01\"  # Required header for Anthropic API\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Alternative Approach:** If Anthropic doesn't provide OpenAI-compatible endpoints, you can still use Langfuse's OpenAI wrapper with a proxy service that translates between OpenAI and Anthropic APIs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example using the standard Anthropic API with Langfuse wrapping\n",
        "from langfuse import Langfuse\n",
        "from anthropic import Anthropic\n",
        "\n",
        "# Initialize Langfuse client\n",
        "langfuse = Langfuse()\n",
        "\n",
        "# Create a trace manually\n",
        "trace = langfuse.trace(name=\"anthropic-chat\")\n",
        "\n",
        "# Initialize Anthropic client\n",
        "anthropic_client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
        "\n",
        "# Create a generation span\n",
        "generation = trace.generation(\n",
        "    name=\"claude-completion\",\n",
        "    model=\"claude-3-5-sonnet-20241022\",\n",
        "    model_parameters={\"max_tokens\": 256, \"temperature\": 0.7}\n",
        ")\n",
        "\n",
        "# Make the API call\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Explain the benefits of using Langfuse with Anthropic's Claude models.\"}\n",
        "]\n",
        "\n",
        "response = anthropic_client.messages.create(\n",
        "    model=\"claude-3-5-sonnet-20241022\",\n",
        "    max_tokens=256,\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Update the generation with response data\n",
        "generation.update(\n",
        "    input=messages,\n",
        "    output=response.content[0].text,\n",
        "    usage={\n",
        "        \"input\": response.usage.input_tokens,\n",
        "        \"output\": response.usage.output_tokens,\n",
        "        \"total\": response.usage.input_tokens + response.usage.output_tokens\n",
        "    },\n",
        "    metadata={\n",
        "        \"stop_reason\": response.stop_reason\n",
        "    }\n",
        ")\n",
        "\n",
        "# End the generation\n",
        "generation.end()\n",
        "\n",
        "print(response.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Features\n",
        "\n",
        "### Streaming Responses\n",
        "\n",
        "Both approaches support streaming responses from Claude:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@observe()\n",
        "def stream_claude_response(prompt: str):\n",
        "    \"\"\"Stream a response from Claude with Langfuse tracing.\"\"\"\n",
        "    \n",
        "    stream = anthropic.messages.create(\n",
        "        model=\"claude-3-5-sonnet-20241022\",\n",
        "        max_tokens=1024,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    full_response = \"\"\n",
        "    for chunk in stream:\n",
        "        if chunk.type == \"content_block_delta\":\n",
        "            text = chunk.delta.text\n",
        "            full_response += text\n",
        "            print(text, end=\"\", flush=True)\n",
        "    \n",
        "    # Update observation after streaming completes\n",
        "    langfuse_context.update_current_observation(\n",
        "        output=full_response,\n",
        "        model=\"claude-3-5-sonnet-20241022\"\n",
        "    )\n",
        "    \n",
        "    return full_response\n",
        "\n",
        "# Example usage\n",
        "response = stream_claude_response(\"Write a haiku about observability.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding Custom Metadata\n",
        "\n",
        "You can enrich your traces with custom metadata for better filtering and analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@observe()\n",
        "def claude_with_metadata(prompt: str, user_id: str, session_id: str):\n",
        "    \"\"\"Call Claude with custom metadata for tracing.\"\"\"\n",
        "    \n",
        "    # Add trace-level metadata\n",
        "    langfuse_context.update_current_trace(\n",
        "        user_id=user_id,\n",
        "        session_id=session_id,\n",
        "        tags=[\"claude\", \"production\"],\n",
        "        metadata={\n",
        "            \"environment\": \"production\",\n",
        "            \"version\": \"1.0.0\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    response = anthropic.messages.create(\n",
        "        model=\"claude-3-5-sonnet-20241022\",\n",
        "        max_tokens=256,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    \n",
        "    # Add observation-level metadata\n",
        "    langfuse_context.update_current_observation(\n",
        "        model=\"claude-3-5-sonnet-20241022\",\n",
        "        input=prompt,\n",
        "        output=response.content[0].text,\n",
        "        metadata={\n",
        "            \"prompt_type\": \"question\",\n",
        "            \"response_format\": \"text\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return response.content[0].text\n",
        "\n",
        "# Example usage\n",
        "result = claude_with_metadata(\n",
        "    prompt=\"What are the key features of Langfuse?\",\n",
        "    user_id=\"user-123\",\n",
        "    session_id=\"session-456\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running these examples, you'll see comprehensive traces in your Langfuse dashboard showing:\n",
        "- Complete request/response data\n",
        "- Token usage and estimated costs\n",
        "- Latency metrics\n",
        "- Custom metadata and tags\n",
        "- Nested trace structure for complex workflows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- Explore [Anthropic's documentation](https://docs.anthropic.com/) to learn more about Claude's capabilities and best practices.\n",
        "- Learn more about [Langfuse tracing features](https://langfuse.com/docs) to track your entire application flow.\n",
        "- Try out Langfuse [Prompt Management](https://langfuse.com/docs/prompts/get-started) to version and manage your Claude prompts.\n",
        "- Set up [LLM-as-a-Judge evaluations](https://langfuse.com/docs/scores/model-based-evals) to automatically evaluate Claude's outputs.\n",
        "- Configure [Langfuse Experiments](https://langfuse.com/docs/experiments) to A/B test different Claude models and parameters.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}